{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-11T17:12:23.139708Z",
     "iopub.status.busy": "2022-03-11T17:12:23.139135Z",
     "iopub.status.idle": "2022-03-11T17:12:27.778033Z",
     "shell.execute_reply": "2022-03-11T17:12:27.777301Z",
     "shell.execute_reply.started": "2022-03-11T17:12:23.139620Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-36c472060a7a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPIL\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "import os\n",
    "import glob as gb\n",
    "import glob\n",
    "import cv2\n",
    "import PIL\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-11T17:12:27.779945Z",
     "iopub.status.busy": "2022-03-11T17:12:27.779711Z",
     "iopub.status.idle": "2022-03-11T17:12:27.783633Z",
     "shell.execute_reply": "2022-03-11T17:12:27.782983Z",
     "shell.execute_reply.started": "2022-03-11T17:12:27.779910Z"
    }
   },
   "outputs": [],
   "source": [
    "#data path\n",
    "trainpath = '/kaggle/input/intel-image-classification/seg_train/seg_train/'\n",
    "testpath = '/kaggle/input/intel-image-classification/seg_test/seg_test/'\n",
    "predpath='/kaggle/input/intel-image-classification/seg_pred/seg_pred'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-11T17:12:27.785750Z",
     "iopub.status.busy": "2022-03-11T17:12:27.785236Z",
     "iopub.status.idle": "2022-03-11T17:12:27.805755Z",
     "shell.execute_reply": "2022-03-11T17:12:27.805109Z",
     "shell.execute_reply.started": "2022-03-11T17:12:27.785713Z"
    }
   },
   "outputs": [],
   "source": [
    "os.listdir(trainpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-11T17:12:27.807753Z",
     "iopub.status.busy": "2022-03-11T17:12:27.807454Z",
     "iopub.status.idle": "2022-03-11T17:12:27.819776Z",
     "shell.execute_reply": "2022-03-11T17:12:27.819162Z",
     "shell.execute_reply.started": "2022-03-11T17:12:27.807717Z"
    }
   },
   "outputs": [],
   "source": [
    "os.listdir(testpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-11T17:12:27.821009Z",
     "iopub.status.busy": "2022-03-11T17:12:27.820721Z",
     "iopub.status.idle": "2022-03-11T17:12:27.825129Z",
     "shell.execute_reply": "2022-03-11T17:12:27.824361Z",
     "shell.execute_reply.started": "2022-03-11T17:12:27.820974Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "img_h = 224\n",
    "img_w = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-11T17:12:27.826825Z",
     "iopub.status.busy": "2022-03-11T17:12:27.826536Z",
     "iopub.status.idle": "2022-03-11T17:12:29.529136Z",
     "shell.execute_reply": "2022-03-11T17:12:29.527965Z",
     "shell.execute_reply.started": "2022-03-11T17:12:27.826792Z"
    }
   },
   "outputs": [],
   "source": [
    "Folder_name=[]\n",
    "folder_item_numbers = []\n",
    "for folder in  os.listdir(trainpath ) : \n",
    "    files = gb.glob(pathname= str( trainpath  + folder + '/*.jpg'))\n",
    "    Folder_name.append(folder)\n",
    "    folder_item_numbers.append(len(files))\n",
    "foldernames=pd.DataFrame({'Folder_name':Folder_name})\n",
    "itemnumbers=pd.DataFrame({'Traning Image Numbers':folder_item_numbers})\n",
    "informations=pd.concat([foldernames,itemnumbers],axis=1)\n",
    "print(informations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-11T17:12:29.530885Z",
     "iopub.status.busy": "2022-03-11T17:12:29.530604Z",
     "iopub.status.idle": "2022-03-11T17:12:30.181743Z",
     "shell.execute_reply": "2022-03-11T17:12:30.180943Z",
     "shell.execute_reply.started": "2022-03-11T17:12:29.530844Z"
    }
   },
   "outputs": [],
   "source": [
    "Folder_name=[]\n",
    "folder_item_numbers = []\n",
    "for folder in  os.listdir(testpath ) : \n",
    "    files = gb.glob(pathname= str( testpath  + folder + '/*.jpg'))\n",
    "    Folder_name.append(folder)\n",
    "    folder_item_numbers.append(len(files))\n",
    "foldernames=pd.DataFrame({'Folder_name':Folder_name})\n",
    "itemnumbers=pd.DataFrame({'Traning Image Numbers':folder_item_numbers})\n",
    "informations=pd.concat([foldernames,itemnumbers],axis=1)\n",
    "print(informations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-11T17:12:30.183317Z",
     "iopub.status.busy": "2022-03-11T17:12:30.183054Z",
     "iopub.status.idle": "2022-03-11T17:12:30.189085Z",
     "shell.execute_reply": "2022-03-11T17:12:30.188435Z",
     "shell.execute_reply.started": "2022-03-11T17:12:30.183280Z"
    }
   },
   "outputs": [],
   "source": [
    "for img in files[:10]:\n",
    "    print(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-11T17:12:30.191634Z",
     "iopub.status.busy": "2022-03-11T17:12:30.190572Z",
     "iopub.status.idle": "2022-03-11T17:12:30.468564Z",
     "shell.execute_reply": "2022-03-11T17:12:30.467928Z",
     "shell.execute_reply.started": "2022-03-11T17:12:30.191585Z"
    }
   },
   "outputs": [],
   "source": [
    "print(Folder_name[0])\n",
    "imge=cv2.imread(files[0])\n",
    "plt.imshow(imge)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-11T17:12:30.471817Z",
     "iopub.status.busy": "2022-03-11T17:12:30.471183Z"
    }
   },
   "outputs": [],
   "source": [
    "#loading data and resize it and collect it in one folder\n",
    "new_size=224    \n",
    "X_train = []\n",
    "y_train = []\n",
    "for folder in  os.listdir(trainpath) : \n",
    "    print( 'folder name is : ', folder)\n",
    "    files = gb.glob(pathname= str( trainpath  + folder + '/*.jpg'))\n",
    "    print( 'numbers of images in folder are : ', len(files))\n",
    "    print(' start reading images')\n",
    "    for file in files: \n",
    "        image_class = {'buildings': 2, 'forest': 4,'glacier': 5 ,'mountain': 0 ,'sea': 3 ,'street':1}\n",
    "        orignal_image = cv2.imread(file)\n",
    "        image = cv2.cvtColor(orignal_image, cv2.COLOR_BGR2RGB)\n",
    "        #image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        resized_image = cv2.resize(image , (new_size,new_size))\n",
    "        X_train.append(resized_image)\n",
    "        y_train.append(image_class[folder])\n",
    "    print('image reading ...finished')\n",
    "print('--------------------------------------------------')        \n",
    "#check items in X_test\n",
    "print(\"items in X_train is:       \",len(X_train) , \" items\") \n",
    "print(\"items in y_train is:       \",len(y_train) , \" items\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#showing training images with labels\n",
    "plt.figure(figsize=(20,20))\n",
    "for n , i in enumerate(list(np.random.randint(0,len(X_train ),16))) : \n",
    "    plt.subplot(4,4,n+1)\n",
    "    plt.imshow(X_train [i])   \n",
    "    plt.axis('off')\n",
    "    classes = {'buildings': 2, 'forest': 4,'glacier': 5 ,'mountain': 0 ,'sea': 3 ,'street':1}\n",
    "    def ImageClass(n):\n",
    "        for x , y in classes.items():\n",
    "            if n == y :\n",
    "                return x\n",
    "    plt.title(ImageClass(y_train[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading data and resize it and collect it in one folder\n",
    "new_size=224    \n",
    "X_test = []\n",
    "y_test = []\n",
    "for folder in  os.listdir(testpath ) : \n",
    "    print( 'folder name is : ', folder)\n",
    "    files = gb.glob(pathname= str( testpath  + folder + '/*.jpg'))\n",
    "    print( 'numbers of images in folder are : ', len(files))\n",
    "    print(' start reading images')\n",
    "    for file in files: \n",
    "        image_class = {'buildings': 2, 'forest': 4,'glacier': 5 ,'mountain': 0 ,'sea': 3 ,'street':1}\n",
    "        orignal_image = cv2.imread(file)\n",
    "        image = cv2.cvtColor(orignal_image, cv2.COLOR_BGR2RGB)\n",
    "        #image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        resized_image = cv2.resize(image , (new_size,new_size))\n",
    "        X_test.append(resized_image)\n",
    "        y_test.append(image_class[folder])\n",
    "#check items in X_test\n",
    "print(\"items in X_test is:       \",len(X_test) , \" items\") \n",
    "print(\"items in y_test is:       \",len(y_test) , \" items\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#showing training images with labels\n",
    "plt.figure(figsize=(20,20))\n",
    "for n , i in enumerate(list(np.random.randint(0,len(X_test),16))) : \n",
    "    plt.subplot(4,4,n+1)\n",
    "    plt.imshow(X_test[i])   \n",
    "    plt.axis('off')\n",
    "    classes ={'buildings': 2, 'forest': 4,'glacier': 5 ,'mountain': 0 ,'sea': 3 ,'street':1}\n",
    "    def ImageClass(n):\n",
    "        for x , y in classes.items():\n",
    "            if n == y :\n",
    "                return x\n",
    "    plt.title(ImageClass(y_test[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "predpath='/kaggle/input/intel-image-classification/seg_pred/seg_pred'\n",
    "new_size=224\n",
    "x_pred=[]\n",
    "files=gb.glob(pathname= str( predpath + '/*.jpg'))\n",
    "\n",
    "for file in files:\n",
    "    imag=cv2.imread(file)\n",
    "   \n",
    "    image = cv2.cvtColor(imag, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    imagee= cv2.resize(image , (new_size,new_size))\n",
    "\n",
    "    x_pred.append(list(imagee))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting all TRAIN data to array\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "print(\"X_train shape  :\" ,X_train.shape)\n",
    "print(\"y_train shape :\", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting all TEST data to array\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "print(\"X_test shape  :\" ,X_test.shape)\n",
    "print(\"y_test shape :\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pred = np.array(x_pred)\n",
    "print(\"x_pred shape  :\" ,x_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model,Sequential\n",
    "from tensorflow.keras.layers import Conv2D,MaxPooling2D,Dense,Flatten,Dropout,BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(200,3, padding = \"same\", activation = \"relu\" ,input_shape=(224, 224, 3)),\n",
    "    MaxPooling2D(5,5),\n",
    "    \n",
    "    Conv2D(150,3, padding = \"same\",activation = \"relu\"),\n",
    "    \n",
    "    Conv2D(120,3, padding = \"same\", activation = \"relu\"),\n",
    "    Dropout(rate=0.5),\n",
    "   \n",
    "    Conv2D(80,3, padding = \"same\",activation = \"relu\"),\n",
    "    \n",
    "    Conv2D(50,3, padding = \"same\", activation = \"relu\"),\n",
    "    MaxPooling2D(5,5),\n",
    "  \n",
    "   \n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(120,activation = \"relu\"),\n",
    "    Dense(100,activation = \"relu\"),\n",
    "    Dense(50,activation = \"relu\"),\n",
    "   \n",
    "    Dropout(rate=0.5),\n",
    "    \n",
    "    Dense(6, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt=tf.keras.optimizers.Adam(0.0001)\n",
    "#compliling model\n",
    "#model.compile(optimizer='adam',loss='categorical_crossentropy' , metrics='accuracy')\n",
    "model.compile(optimizer=opt,loss='sparse_categorical_crossentropy' , metrics='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, CSVLogger, LearningRateScheduler,TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystop=EarlyStopping(patience=15)\n",
    "filepath = \"model.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1,save_best_only=True, mode='max')\n",
    "log_fname = 'model_log.csv'\n",
    "csv_logger = CSVLogger(filename=log_fname,separator=',',append=False)\n",
    "callbacks_list = [checkpoint, csv_logger,earlystop] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape,y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "test_data_gen  = ImageDataGenerator()\n",
    "test = test_data_gen.flow(\n",
    "      X_test,\n",
    "      y_test,\n",
    "      shuffle=True, \n",
    "      batch_size=batch_size\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history= model.fit(X_train,y_train,validation_data=test,epochs=25,verbose=1,batch_size=32,callbacks=[callbacks_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#showing results and model accuracy \n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "#epochs=30\n",
    "epochs_range = range(25)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('model.h5')\n",
    "metrics = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(X_test)\n",
    "prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('model.h5')\n",
    "prediction = model.predict(x_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "for n , i in enumerate(list(np.random.randint(0,len(x_pred),16))) : \n",
    "    plt.subplot(4,4,n+1)\n",
    "    plt.imshow(x_pred[i])\n",
    "    plt.axis('off')\n",
    "    plt.title(ImageClass(np.argmax(prediction[i])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
